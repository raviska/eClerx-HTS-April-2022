{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oxDekwPzdDVE",
    "outputId": "0d29a69c-f7be-45ca-9843-c9ad6b529ffc"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp39-cp39-win_amd64.whl (438.0 MB)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\16020\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\16020\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\16020\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.3)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\16020\\anaconda3\\lib\\site-packages (from tensorflow) (3.10.0.2)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-13.0.0-py2.py3-none-win_amd64.whl (13.9 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\16020\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Collecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\16020\\anaconda3\\lib\\site-packages (from tensorflow) (58.0.4)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.44.0-cp39-cp39-win_amd64.whl (3.4 MB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.20.0-cp39-cp39-win_amd64.whl (904 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\16020\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\16020\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.26.0)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.5-py2.py3-none-any.whl (156 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\16020\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\16020\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\16020\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.6.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\16020\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\16020\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\16020\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\16020\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.2)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=585ea6c87910fae686cf704824a49dee56248749105880a9be4ef4e6464c3d92\n",
      "  Stored in directory: c:\\users\\16020\\appdata\\local\\pip\\cache\\wheels\\b6\\0d\\90\\0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.5 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 keras-2.8.0 keras-preprocessing-1.1.2 libclang-13.0.0 markdown-3.3.6 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.20.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "Cen8Qp5bdXco",
    "outputId": "e5f4dcee-e269-4ca0-e88e-6727c3919618"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16020\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (2,3,4,5,6,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Global_active_power</th>\n",
       "      <th>Global_reactive_power</th>\n",
       "      <th>Voltage</th>\n",
       "      <th>Global_intensity</th>\n",
       "      <th>Sub_metering_1</th>\n",
       "      <th>Sub_metering_2</th>\n",
       "      <th>Sub_metering_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:24:00</td>\n",
       "      <td>4.216</td>\n",
       "      <td>0.418</td>\n",
       "      <td>234.840</td>\n",
       "      <td>18.400</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:25:00</td>\n",
       "      <td>5.360</td>\n",
       "      <td>0.436</td>\n",
       "      <td>233.630</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:26:00</td>\n",
       "      <td>5.374</td>\n",
       "      <td>0.498</td>\n",
       "      <td>233.290</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:27:00</td>\n",
       "      <td>5.388</td>\n",
       "      <td>0.502</td>\n",
       "      <td>233.740</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:28:00</td>\n",
       "      <td>3.666</td>\n",
       "      <td>0.528</td>\n",
       "      <td>235.680</td>\n",
       "      <td>15.800</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Time Global_active_power Global_reactive_power  Voltage  \\\n",
       "0  16/12/2006  17:24:00               4.216                 0.418  234.840   \n",
       "1  16/12/2006  17:25:00               5.360                 0.436  233.630   \n",
       "2  16/12/2006  17:26:00               5.374                 0.498  233.290   \n",
       "3  16/12/2006  17:27:00               5.388                 0.502  233.740   \n",
       "4  16/12/2006  17:28:00               3.666                 0.528  235.680   \n",
       "\n",
       "  Global_intensity Sub_metering_1 Sub_metering_2  Sub_metering_3  \n",
       "0           18.400          0.000          1.000            17.0  \n",
       "1           23.000          0.000          1.000            16.0  \n",
       "2           23.000          0.000          2.000            17.0  \n",
       "3           23.000          0.000          1.000            17.0  \n",
       "4           15.800          0.000          1.000            17.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import packages\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n",
    "# read the dataset into python\n",
    "df = pd.read_csv(r\"C:\\Users\\16020\\Downloads\\household_power_consumption.txt\\household_power_consumption.txt\", delimiter=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ocZDn3O5ewy8",
    "outputId": "9c4d1e8d-e884-43d3-bda7-5f452df7a302"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns after removing missing values: (2049280, 2)\n",
      "The time series starts from:  2006-12-16 17:24:00\n",
      "The time series ends on:  2010-12-11 23:59:00\n",
      "Wall time: 5min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#\n",
    "df['date_time'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "df['Global_active_power'] = pd.to_numeric(df['Global_active_power'], errors='coerce')\n",
    "df = df.dropna(subset=['Global_active_power'])\n",
    "\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "\n",
    "df = df.loc[:, ['date_time', 'Global_active_power']]\n",
    "df.sort_values('date_time', inplace=True, ascending=True)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print('Number of rows and columns after removing missing values:', df.shape)\n",
    "print('The time series starts from: ', df['date_time'].min())\n",
    "print('The time series ends on: ', df['date_time'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "id": "TtT9D9FCe8eD",
    "outputId": "652124f4-9d8b-4fa6-9084-677be71f3a07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2049280 entries, 0 to 2049279\n",
      "Data columns (total 2 columns):\n",
      " #   Column               Dtype         \n",
      "---  ------               -----         \n",
      " 0   date_time            datetime64[ns]\n",
      " 1   Global_active_power  float64       \n",
      "dtypes: datetime64[ns](1), float64(1)\n",
      "memory usage: 31.3 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>Global_active_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-12-16 17:24:00</td>\n",
       "      <td>4.216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-12-16 17:25:00</td>\n",
       "      <td>5.360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-12-16 17:26:00</td>\n",
       "      <td>5.374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-12-16 17:27:00</td>\n",
       "      <td>5.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-12-16 17:28:00</td>\n",
       "      <td>3.666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2006-12-16 17:29:00</td>\n",
       "      <td>3.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2006-12-16 17:30:00</td>\n",
       "      <td>3.702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2006-12-16 17:31:00</td>\n",
       "      <td>3.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2006-12-16 17:32:00</td>\n",
       "      <td>3.668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2006-12-16 17:33:00</td>\n",
       "      <td>3.662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_time  Global_active_power\n",
       "0 2006-12-16 17:24:00                4.216\n",
       "1 2006-12-16 17:25:00                5.360\n",
       "2 2006-12-16 17:26:00                5.374\n",
       "3 2006-12-16 17:27:00                5.388\n",
       "4 2006-12-16 17:28:00                3.666\n",
       "5 2006-12-16 17:29:00                3.520\n",
       "6 2006-12-16 17:30:00                3.702\n",
       "7 2006-12-16 17:31:00                3.700\n",
       "8 2006-12-16 17:32:00                3.668\n",
       "9 2006-12-16 17:33:00                3.662"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fAHkwu52fBbI",
    "outputId": "c36824e5-8952-47d3-94d8-f51eb49538f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dates: 2010-12-05 00:00:00 to 2010-12-11 23:59:00\n",
      "Validation dates: 2010-11-21 00:00:00 to 2010-12-04 23:59:00\n",
      "Train dates: 2006-12-16 17:24:00 to 2010-11-20 23:59:00\n"
     ]
    }
   ],
   "source": [
    "# Split into training, validation and test datasets.\n",
    "# Since it's timeseries we should do it by date.\n",
    "test_cutoff_date = df['date_time'].max() - timedelta(days=7)\n",
    "val_cutoff_date = test_cutoff_date - timedelta(days=14)\n",
    "\n",
    "df_test = df[df['date_time'] > test_cutoff_date]\n",
    "df_val = df[(df['date_time'] > val_cutoff_date) & (df['date_time'] <= test_cutoff_date)]\n",
    "df_train = df[df['date_time'] <= val_cutoff_date]\n",
    "\n",
    "#check out the datasets\n",
    "print('Test dates: {} to {}'.format(df_test['date_time'].min(), df_test['date_time'].max()))\n",
    "print('Validation dates: {} to {}'.format(df_val['date_time'].min(), df_val['date_time'].max()))\n",
    "print('Train dates: {} to {}'.format(df_train['date_time'].min(), df_train['date_time'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mnAydIi0fMTb"
   },
   "outputs": [],
   "source": [
    "# Goal of the model:\n",
    "#  Predict Global_active_power at a specified time in the future.\n",
    "#   Eg. We want to predict how much Global_active_power will be ten minutes from now.\n",
    "#       We can use all the values from t-1, t-2, t-3, .... t-history_length to predict t+10\n",
    "\n",
    "\n",
    "def create_ts_files(dataset, \n",
    "                    start_index, \n",
    "                    end_index, \n",
    "                    history_length, \n",
    "                    step_size, \n",
    "                    target_step, \n",
    "                    num_rows_per_file, \n",
    "                    data_folder):\n",
    "    assert step_size > 0\n",
    "    assert start_index >= 0\n",
    "    \n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "    \n",
    "    time_lags = sorted(range(target_step+1, target_step+history_length+1, step_size), reverse=True)\n",
    "    col_names = [f'x_lag{i}' for i in time_lags] + ['y']\n",
    "    start_index = start_index + history_length\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_step\n",
    "    \n",
    "    rng = range(start_index, end_index)\n",
    "    num_rows = len(rng)\n",
    "    num_files = math.ceil(num_rows/num_rows_per_file)\n",
    "    \n",
    "    # for each file.\n",
    "    print(f'Creating {num_files} files.')\n",
    "    for i in range(num_files):\n",
    "        filename = f'{data_folder}/ts_file{i}.pkl'\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'{filename}')\n",
    "            \n",
    "        # get the start and end indices.\n",
    "        ind0 = i*num_rows_per_file\n",
    "        ind1 = min(ind0 + num_rows_per_file, end_index)\n",
    "        data_list = []\n",
    "        \n",
    "        # j in the current timestep. Will need j-n to j-1 for the history. And j + target_step for the target.\n",
    "        for j in range(ind0, ind1):\n",
    "            indices = range(j-1, j-history_length-1, -step_size)\n",
    "            data = dataset[sorted(indices) + [j+target_step]]\n",
    "            \n",
    "            # append data to the list.\n",
    "            data_list.append(data)\n",
    "\n",
    "        df_ts = pd.DataFrame(data=data_list, columns=col_names)\n",
    "        df_ts.to_pickle(filename)\n",
    "            \n",
    "    return len(col_names)-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9kVtsawxfXm2",
    "outputId": "431aa2b9-675b-484b-e781-13c30b1b73b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 158 files.\n",
      "ts_data/ts_file0.pkl\n",
      "ts_data/ts_file10.pkl\n",
      "ts_data/ts_file20.pkl\n",
      "ts_data/ts_file30.pkl\n",
      "ts_data/ts_file40.pkl\n",
      "ts_data/ts_file50.pkl\n",
      "ts_data/ts_file60.pkl\n",
      "ts_data/ts_file70.pkl\n",
      "ts_data/ts_file80.pkl\n",
      "ts_data/ts_file90.pkl\n",
      "ts_data/ts_file100.pkl\n",
      "ts_data/ts_file110.pkl\n",
      "ts_data/ts_file120.pkl\n",
      "ts_data/ts_file130.pkl\n",
      "ts_data/ts_file140.pkl\n",
      "ts_data/ts_file150.pkl\n",
      "Wall time: 32min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "global_active_power = df_train['Global_active_power'].values\n",
    "\n",
    "# Scaled to work with Neural networks.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "global_active_power_scaled = scaler.fit_transform(global_active_power.reshape(-1, 1)).reshape(-1, )\n",
    "\n",
    "history_length = 7*24*60  # The history length in minutes.\n",
    "step_size = 10  # The sampling rate of the history. Eg. If step_size = 1, then values from every minute will be in the history.\n",
    "                #                                       If step size = 10 then values every 10 minutes will be in the history.\n",
    "target_step = 10  # The time step in the future to predict. Eg. If target_step = 0, then predict the next timestep after the end of the history period.\n",
    "                  #                                             If target_step = 10 then predict 10 timesteps the next timestep (11 minutes after the end of history).\n",
    "\n",
    "# The csv creation returns the number of rows and number of features. We need these values below.\n",
    "num_timesteps = create_ts_files(global_active_power_scaled,\n",
    "                                start_index=0,\n",
    "                                end_index=None,\n",
    "                                history_length=history_length,\n",
    "                                step_size=step_size,\n",
    "                                target_step=target_step,\n",
    "                                num_rows_per_file=128*100,\n",
    "                                data_folder='ts_data')\n",
    "\n",
    "# I found that the easiest way to do time series with tensorflow is by creating pandas files with the lagged time steps (eg. x{t-1}, x{t-2}...) and \n",
    "# the value to predict y = x{t+n}. We tried doing it using TFRecords, but that API is not very intuitive and lacks working examples for time series.\n",
    "# The resulting file using these parameters is over 17GB. If history_length is increased, or  step_size is decreased, it could get much bigger.\n",
    "# Hard to fit into laptop memory, so need to use other means to load the data from the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kWtzmWNwffIa"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# So we can handle loading the data in chunks from the hard drive instead of having to load everything into memory.\n",
    "# \n",
    "# The reason we want to do this is so we can do custom processing on the data that we are feeding into the LSTM.\n",
    "# LSTM requires a certain shape and it is tricky to get it right.\n",
    "#\n",
    "class TimeSeriesLoader:\n",
    "    def __init__(self, ts_folder, filename_format):\n",
    "        self.ts_folder = ts_folder\n",
    "        \n",
    "        # find the number of files.\n",
    "        i = 0\n",
    "        file_found = True\n",
    "        while file_found:\n",
    "            filename = self.ts_folder + '/' + filename_format.format(i)\n",
    "            file_found = os.path.exists(filename)\n",
    "            if file_found:\n",
    "                i += 1\n",
    "                \n",
    "        self.num_files = i\n",
    "        self.files_indices = np.arange(self.num_files)\n",
    "        self.shuffle_chunks()\n",
    "        \n",
    "    def num_chunks(self):\n",
    "        return self.num_files\n",
    "    \n",
    "    def get_chunk(self, idx):\n",
    "        assert (idx >= 0) and (idx < self.num_files)\n",
    "        \n",
    "        ind = self.files_indices[idx]\n",
    "        filename = self.ts_folder + '/' + filename_format.format(ind)\n",
    "        df_ts = pd.read_pickle(filename)\n",
    "        num_records = len(df_ts.index)\n",
    "        \n",
    "        features = df_ts.drop('y', axis=1).values\n",
    "        target = df_ts['y'].values\n",
    "        \n",
    "        # reshape for input into LSTM. Batch major format.\n",
    "        features_batchmajor = np.array(features).reshape(num_records, -1, 1)\n",
    "        return features_batchmajor, target\n",
    "    \n",
    "    # this shuffles the order the chunks will be outputted from get_chunk.\n",
    "    def shuffle_chunks(self):\n",
    "        np.random.shuffle(self.files_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9enBzFiHfZwE"
   },
   "outputs": [],
   "source": [
    "ts_folder = 'ts_data'\n",
    "filename_format = 'ts_file{}.pkl'\n",
    "tss = TimeSeriesLoader(ts_folder, filename_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7BRZnsASfq2Q"
   },
   "outputs": [],
   "source": [
    "# Create the Keras model.\n",
    "# Use hyperparameter optimization if you have the time.\n",
    "\n",
    "ts_inputs = tf.keras.Input(shape=(num_timesteps, 1))\n",
    "\n",
    "# units=10 -> The cell and hidden states will be of dimension 10.\n",
    "#             The number of parameters that need to be trained = 4*units*(units+2)\n",
    "x = layers.LSTM(units=10)(ts_inputs)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(1, activation='linear')(x)\n",
    "model = tf.keras.Model(inputs=ts_inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cX3hnOSHf7fx"
   },
   "outputs": [],
   "source": [
    "# Specify the training configuration.\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r34U9daqf7ip",
    "outputId": "65f6ee0c-8d50-49f5-f2d9-710bf5e1efa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1008, 1)]         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 10)                480       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 491\n",
      "Trainable params: 491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10VK0wP_f7ln",
    "outputId": "52fe3cbe-34a5-4ade-e082-2592dee77395"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\n",
      "100/100 [==============================] - 34s 312ms/step - loss: 0.0125 - mse: 0.0125\n",
      "100/100 [==============================] - 31s 307ms/step - loss: 0.0163 - mse: 0.0163\n",
      "100/100 [==============================] - 30s 304ms/step - loss: 0.0101 - mse: 0.0101\n",
      "100/100 [==============================] - 31s 308ms/step - loss: 0.0110 - mse: 0.0110\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 0.0272 - mse: 0.0272\n",
      "100/100 [==============================] - 31s 310ms/step - loss: 0.0142 - mse: 0.0142\n",
      "100/100 [==============================] - 31s 307ms/step - loss: 0.0157 - mse: 0.0157\n",
      "100/100 [==============================] - 31s 310ms/step - loss: 0.0093 - mse: 0.0093\n",
      "100/100 [==============================] - 31s 311ms/step - loss: 0.0073 - mse: 0.0073\n",
      "100/100 [==============================] - 32s 323ms/step - loss: 0.0197 - mse: 0.0197\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 0.0170 - mse: 0.0170\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.0086 - mse: 0.0086\n",
      "100/100 [==============================] - 37s 369ms/step - loss: 0.0118 - mse: 0.0118\n",
      "100/100 [==============================] - 37s 369ms/step - loss: 0.0109 - mse: 0.0109\n",
      "100/100 [==============================] - 37s 369ms/step - loss: 0.0078 - mse: 0.0078\n",
      "100/100 [==============================] - 37s 369ms/step - loss: 0.0123 - mse: 0.0123\n",
      "100/100 [==============================] - 37s 367ms/step - loss: 0.0123 - mse: 0.0123\n",
      "100/100 [==============================] - 37s 368ms/step - loss: 0.0076 - mse: 0.0076\n",
      "100/100 [==============================] - 37s 370ms/step - loss: 0.0082 - mse: 0.0082\n",
      "100/100 [==============================] - 37s 369ms/step - loss: 0.0072 - mse: 0.0072\n",
      "100/100 [==============================] - 37s 370ms/step - loss: 0.0072 - mse: 0.0072\n",
      "100/100 [==============================] - 37s 372ms/step - loss: 0.0075 - mse: 0.0075\n",
      "100/100 [==============================] - 37s 370ms/step - loss: 0.0061 - mse: 0.0061\n",
      "100/100 [==============================] - 37s 372ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 37s 374ms/step - loss: 0.0108 - mse: 0.0108\n",
      "100/100 [==============================] - 37s 372ms/step - loss: 0.0073 - mse: 0.0073\n",
      "100/100 [==============================] - 37s 370ms/step - loss: 0.0076 - mse: 0.0076\n",
      "100/100 [==============================] - 37s 372ms/step - loss: 0.0088 - mse: 0.0088\n",
      "100/100 [==============================] - 37s 372ms/step - loss: 0.0106 - mse: 0.0106\n",
      "100/100 [==============================] - 37s 373ms/step - loss: 0.0077 - mse: 0.0077\n",
      "100/100 [==============================] - 37s 374ms/step - loss: 0.0094 - mse: 0.0094\n",
      "100/100 [==============================] - 37s 374ms/step - loss: 0.0060 - mse: 0.0060\n",
      "100/100 [==============================] - 37s 373ms/step - loss: 0.0068 - mse: 0.0068\n",
      "100/100 [==============================] - 37s 374ms/step - loss: 0.0091 - mse: 0.0091\n",
      "100/100 [==============================] - 38s 375ms/step - loss: 0.0067 - mse: 0.0067\n",
      "100/100 [==============================] - 37s 373ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 37s 371ms/step - loss: 0.0082 - mse: 0.0082\n",
      "100/100 [==============================] - 37s 372ms/step - loss: 0.0072 - mse: 0.0072\n",
      "100/100 [==============================] - 37s 373ms/step - loss: 0.0115 - mse: 0.0115\n",
      "100/100 [==============================] - 37s 373ms/step - loss: 0.0067 - mse: 0.0067\n",
      "100/100 [==============================] - 38s 375ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 37s 374ms/step - loss: 0.0070 - mse: 0.0070\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 37s 374ms/step - loss: 0.0093 - mse: 0.0093\n",
      "100/100 [==============================] - 38s 375ms/step - loss: 0.0057 - mse: 0.0057\n",
      "100/100 [==============================] - 38s 376ms/step - loss: 0.0035 - mse: 0.0035\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0075 - mse: 0.0075\n",
      "100/100 [==============================] - 38s 376ms/step - loss: 0.0079 - mse: 0.0079\n",
      "100/100 [==============================] - 38s 375ms/step - loss: 0.0069 - mse: 0.0069\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0065 - mse: 0.0065\n",
      "100/100 [==============================] - 37s 374ms/step - loss: 0.0063 - mse: 0.0063\n",
      "100/100 [==============================] - 38s 376ms/step - loss: 0.0082 - mse: 0.0082\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0057 - mse: 0.0057\n",
      "100/100 [==============================] - 38s 378ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 38s 378ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 38s 380ms/step - loss: 0.0075 - mse: 0.0075\n",
      "100/100 [==============================] - 38s 380ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 38s 381ms/step - loss: 0.0025 - mse: 0.0025\n",
      "100/100 [==============================] - 38s 379ms/step - loss: 0.0087 - mse: 0.0087\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 38s 378ms/step - loss: 0.0062 - mse: 0.0062\n",
      "100/100 [==============================] - 38s 379ms/step - loss: 0.0083 - mse: 0.0083\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0056 - mse: 0.0056\n",
      "100/100 [==============================] - 38s 379ms/step - loss: 0.0060 - mse: 0.0060\n",
      "100/100 [==============================] - 38s 379ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0041 - mse: 0.0041\n",
      "100/100 [==============================] - 38s 379ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 38s 385ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 38s 376ms/step - loss: 0.0054 - mse: 0.0054\n",
      "100/100 [==============================] - 38s 376ms/step - loss: 0.0041 - mse: 0.0041\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0063 - mse: 0.0063\n",
      "100/100 [==============================] - 37s 373ms/step - loss: 0.0063 - mse: 0.0063\n",
      "100/100 [==============================] - 38s 379ms/step - loss: 0.0051 - mse: 0.0051\n",
      "100/100 [==============================] - 38s 379ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0023 - mse: 0.0023\n",
      "100/100 [==============================] - 38s 375ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 38s 379ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0089 - mse: 0.0089\n",
      "100/100 [==============================] - 39s 388ms/step - loss: 0.0020 - mse: 0.0020\n",
      "100/100 [==============================] - 38s 378ms/step - loss: 0.0061 - mse: 0.0061\n",
      "100/100 [==============================] - 38s 378ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 38s 381ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 38s 375ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 38s 376ms/step - loss: 0.0070 - mse: 0.0070\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 37s 373ms/step - loss: 0.0023 - mse: 0.0023\n",
      "100/100 [==============================] - 38s 375ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 37s 375ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 38s 379ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 38s 375ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 38s 376ms/step - loss: 0.0021 - mse: 0.0021\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 37s 365ms/step - loss: 0.0067 - mse: 0.0067\n",
      "100/100 [==============================] - 38s 375ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 37s 375ms/step - loss: 0.0068 - mse: 0.0068\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 38s 376ms/step - loss: 0.0071 - mse: 0.0071\n",
      "100/100 [==============================] - 38s 375ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0031 - mse: 0.0031\n",
      "100/100 [==============================] - 38s 381ms/step - loss: 0.0062 - mse: 0.0062\n",
      "100/100 [==============================] - 38s 376ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0016 - mse: 0.0016\n",
      "100/100 [==============================] - 38s 376ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 38s 376ms/step - loss: 0.0085 - mse: 0.0085\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0051 - mse: 0.0051\n",
      "100/100 [==============================] - 38s 378ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 37s 373ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 37s 375ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 38s 375ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 38s 376ms/step - loss: 0.0061 - mse: 0.0061\n",
      "100/100 [==============================] - 37s 369ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 37s 374ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 38s 376ms/step - loss: 0.0023 - mse: 0.0023\n",
      "100/100 [==============================] - 37s 373ms/step - loss: 0.0035 - mse: 0.0035\n",
      "100/100 [==============================] - 37s 371ms/step - loss: 0.0041 - mse: 0.0041\n",
      "100/100 [==============================] - 37s 372ms/step - loss: 0.0034 - mse: 0.0034\n",
      "100/100 [==============================] - 37s 373ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0066 - mse: 0.0066\n",
      "100/100 [==============================] - 37s 373ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0063 - mse: 0.0063\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 37s 372ms/step - loss: 0.0059 - mse: 0.0059\n",
      "100/100 [==============================] - 37s 373ms/step - loss: 0.0054 - mse: 0.0054\n",
      "100/100 [==============================] - 37s 372ms/step - loss: 0.0035 - mse: 0.0035\n",
      "100/100 [==============================] - 37s 371ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 38s 380ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 37s 375ms/step - loss: 0.0053 - mse: 0.0053\n",
      "100/100 [==============================] - 37s 374ms/step - loss: 0.0032 - mse: 0.0032\n",
      "100/100 [==============================] - 38s 375ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 0.0053 - mse: 0.0053\n",
      "100/100 [==============================] - 21s 213ms/step - loss: 0.0033 - mse: 0.0033\n",
      "100/100 [==============================] - 21s 207ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 22s 219ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 22s 222ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 22s 215ms/step - loss: 0.0067 - mse: 0.0067\n",
      "100/100 [==============================] - 22s 215ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 23s 230ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 23s 229ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 23s 231ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 23s 231ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 23s 231ms/step - loss: 0.0035 - mse: 0.0035\n",
      "100/100 [==============================] - 23s 230ms/step - loss: 0.0032 - mse: 0.0032\n",
      "100/100 [==============================] - 23s 228ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 22s 216ms/step - loss: 0.0033 - mse: 0.0033\n",
      "100/100 [==============================] - 23s 225ms/step - loss: 0.0013 - mse: 0.0013\n",
      "100/100 [==============================] - 23s 234ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 23s 230ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.0051 - mse: 0.0051\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 5.0982e-04 - mse: 5.0982e-04\n",
      "Wall time: 1h 33min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train in batch sizes of 128.\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 1\n",
    "NUM_CHUNKS = tss.num_chunks()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('epoch #{}'.format(epoch))\n",
    "    for i in range(NUM_CHUNKS):\n",
    "        X, y = tss.get_chunk(i)\n",
    "        \n",
    "        # model.fit does train the model incrementally. ie. Can call multiple times in batches.\n",
    "        # https://github.com/keras-team/keras/issues/4446\n",
    "        model.fit(x=X, y=y, batch_size=BATCH_SIZE)\n",
    "        \n",
    "    # shuffle the chunks so they're not in the same order next time around.\n",
    "    tss.shuffle_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "yYwY2eTff7oU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 1 files.\n",
      "ts_val_data/ts_file0.pkl\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the validation set.\n",
    "#\n",
    "# Create the validation CSV like we did before with the training.\n",
    "global_active_power_val = df_val['Global_active_power'].values\n",
    "global_active_power_val_scaled = scaler.transform(global_active_power_val.reshape(-1, 1)).reshape(-1, )\n",
    "\n",
    "history_length = 7*24*60  # The history length in minutes.\n",
    "step_size = 10  # The sampling rate of the history. Eg. If step_size = 1, then values from every minute will be in the history.\n",
    "                #                                       If step size = 10 then values every 10 minutes will be in the history.\n",
    "target_step = 10  # The time step in the future to predict. Eg. If target_step = 0, then predict the next timestep after the end of the history period.\n",
    "                  #                                             If target_step = 10 then predict 10 timesteps the next timestep (11 minutes after the end of history).\n",
    "\n",
    "# The csv creation returns the number of rows and number of features. We need these values below.\n",
    "num_timesteps = create_ts_files(global_active_power_val_scaled,\n",
    "                                start_index=0,\n",
    "                                end_index=None,\n",
    "                                history_length=history_length,\n",
    "                                step_size=step_size,\n",
    "                                target_step=target_step,\n",
    "                                num_rows_per_file=128*100,\n",
    "                                data_folder='ts_val_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1NFFiI05f7rU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation mean squared error: 0.45337705282844565\n",
      "validation baseline mean squared error: 0.428345914375\n"
     ]
    }
   ],
   "source": [
    "# If we assume that the validation dataset can fit into memory we can do this.\n",
    "df_val_ts = pd.read_pickle('ts_val_data/ts_file0.pkl')\n",
    "\n",
    "\n",
    "features = df_val_ts.drop('y', axis=1).values\n",
    "features_arr = np.array(features)\n",
    "\n",
    "# reshape for input into LSTM. Batch major format.\n",
    "num_records = len(df_val_ts.index)\n",
    "features_batchmajor = features_arr.reshape(num_records, -1, 1)\n",
    "\n",
    "\n",
    "y_pred = model.predict(features_batchmajor).reshape(-1, )\n",
    "y_pred = scaler.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1 ,)\n",
    "\n",
    "y_act = df_val_ts['y'].values\n",
    "y_act = scaler.inverse_transform(y_act.reshape(-1, 1)).reshape(-1 ,)\n",
    "\n",
    "print('validation mean squared error: {}'.format(mean_squared_error(y_act, y_pred)))\n",
    "\n",
    "#baseline\n",
    "y_pred_baseline = df_val_ts['x_lag11'].values\n",
    "y_pred_baseline = scaler.inverse_transform(y_pred_baseline.reshape(-1, 1)).reshape(-1 ,)\n",
    "print('validation baseline mean squared error: {}'.format(mean_squared_error(y_act, y_pred_baseline)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2QHTy2Hf7uY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muwy4Azhf7x1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
